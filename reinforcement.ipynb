{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e16c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import RL_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad67955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our existing CNN model\n",
    "ref_model_path = \"models/CNN_ResNet.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bfe58",
   "metadata": {},
   "source": [
    "## Load stockfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3143cc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PositionEvaluator' from 'RL_utils' (/home/smith3_j@WMGDS.WMG.WARWICK.AC.UK/Documents/jordi-lete/Chess-AI/RL_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRL_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositionEvaluator\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PositionEvaluator' from 'RL_utils' (/home/smith3_j@WMGDS.WMG.WARWICK.AC.UK/Documents/jordi-lete/Chess-AI/RL_utils.py)"
     ]
    }
   ],
   "source": [
    "from RL_utils import PositionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df9168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stockfish initialized with ELO: 1400\n"
     ]
    }
   ],
   "source": [
    "stockfish_path = \"models/stockfish/stockfish-windows-x86-64-avx2.exe\"\n",
    "evaluator = PositionEvaluator(stockfish_path, elo_rating=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b4ef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Move.from_uci('h7h6')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = chess.Board()\n",
    "board.push_uci(\"e2e4\")\n",
    "board.push_uci(\"h7h6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8d0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position evaluation: 0.83\n"
     ]
    }
   ],
   "source": [
    "eval_score = evaluator.evaluate_position(board)\n",
    "print(f\"Position evaluation: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364ac607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best moves: [{'Move': 'd2d4', 'Centipawn': 83, 'Mate': None}, {'Move': 'g1f3', 'Centipawn': 71, 'Mate': None}, {'Move': 'b1c3', 'Centipawn': 68, 'Mate': None}]\n"
     ]
    }
   ],
   "source": [
    "best_moves = evaluator.get_best_moves(board, 3)\n",
    "print(f\"Best moves: {best_moves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f434f5",
   "metadata": {},
   "source": [
    "## Load our CNN trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46049a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_model, device = RL_utils.load_model(ref_model_path)\n",
    "# reference_model = reference_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605a6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# dummy_input = torch.randn(1, 19, 8, 8).to(device)\n",
    "# with torch.no_grad():\n",
    "#     output = model(dummy_input)\n",
    "#     print(f\"Model output shape: {output.shape}\")  # Should be [1, 4288]\n",
    "#     print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7e154",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2570efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1000 varied positions (openings, early, middlegame)\n",
      "Total positions loaded: 1000\n"
     ]
    }
   ],
   "source": [
    "pgn_file = \"games/lichess_db_2016-04.pgn\"\n",
    "positions = []\n",
    "positions += RL_utils.extract_varied_positions(pgn_file, num_positions=1000)\n",
    "print(f\"Total positions loaded: {len(positions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ea359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creating a batch\n",
    "# if positions:\n",
    "#     board_tensors, legal_masks, boards = RL_utils.create_training_batch(positions, batch_size=4)\n",
    "#     print(f\"Batch shapes:\")\n",
    "#     print(f\"  Board tensors: {board_tensors.shape}\")\n",
    "#     print(f\"  Legal masks: {legal_masks.shape}\")\n",
    "#     print(f\"  Number of boards: {len(boards)}\")\n",
    "    \n",
    "#     # Show a sample position\n",
    "#     print(f\"\\nSample position FEN: {boards[0].fen()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3324c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boards[0]\n",
    "# eval_score = evaluator.evaluate_position(boards[0])\n",
    "# print(f\"Position evaluation: {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5400975",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1ab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "976b0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "\n",
    "# Self-play\n",
    "NUM_SELF_PLAY_GAMES = 100\n",
    "MAX_GAME_MOVES = 160\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "# Model Saving\n",
    "MODEL_SAVE_PATH = \"models/rlmodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f92ff69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_self_play_data(model, device, start_positions, num_games=NUM_SELF_PLAY_GAMES):\n",
    "    \"\"\"Generate self-play data with error handling\"\"\"\n",
    "    data = []\n",
    "    successful_games = 0\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        try:\n",
    "            board = random.choice(start_positions)\n",
    "            # game_history, result = RL_utils.play_self_play_game(model, device, board, MAX_GAME_MOVES, TEMPERATURE)\n",
    "            game_history, result = RL_utils.play_game_with_mcts(model, device, board, MAX_GAME_MOVES, TEMPERATURE)\n",
    "            \n",
    "            if len(game_history) > 0:  # Only add if we have valid data\n",
    "                data.append((game_history, result))\n",
    "                successful_games += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in game {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Generated {successful_games} successful games out of {num_games} attempts\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83df7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_self_play(model, optimizer, game_histories, device):\n",
    "    \"\"\"Train model on self-play data with gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Flatten all examples from all games\n",
    "    all_examples = []\n",
    "    for history, result in game_histories:\n",
    "        for board_tensor, legal_mask, move_idx, turn in history:\n",
    "            # Convert result to value from current player's perspective\n",
    "            value = result if turn else -result\n",
    "            all_examples.append((board_tensor, legal_mask, move_idx, value))\n",
    "    \n",
    "    if len(all_examples) == 0:\n",
    "        print(\"No training examples available!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Training on {len(all_examples)} examples\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        random.shuffle(all_examples)\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for i in range(0, len(all_examples), BATCH_SIZE):\n",
    "            batch = all_examples[i:i+BATCH_SIZE]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch\n",
    "                boards = torch.cat([ex[0] for ex in batch]).to(device)\n",
    "                masks = torch.stack([ex[1] for ex in batch]).to(device)\n",
    "                move_targets = torch.tensor([ex[2] for ex in batch], dtype=torch.long).to(device)\n",
    "                value_targets = torch.tensor([ex[3] for ex in batch], dtype=torch.float).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                policy_logits, value_preds = model(boards)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, p_loss, v_loss = RL_utils.compute_loss(policy_logits, value_preds, move_targets, value_targets, masks)\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Skipping batch due to NaN loss\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                epoch_losses.append(loss.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//BATCH_SIZE}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if epoch_losses:\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: No valid batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f5f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_on_mcts(reference_model, model, optimizer, game_histories, device):\n",
    "    \"\"\"Train model on self-play data with gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Flatten all examples from all games\n",
    "    all_examples = []\n",
    "    for history, result in game_histories:\n",
    "        for board_tensor, policy_vector, turn in history:\n",
    "            # Convert result to value from current player's perspective\n",
    "            value = result if turn else -result\n",
    "            policy_tensor = torch.tensor(policy_vector, dtype=torch.float32)\n",
    "            all_examples.append((board_tensor, policy_tensor, value))\n",
    "    \n",
    "    if len(all_examples) == 0:\n",
    "        print(\"No training examples available!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Training on {len(all_examples)} examples\")\n",
    "    \n",
    "    kl_history = []\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        random.shuffle(all_examples)\n",
    "        batch_kl = []\n",
    "        batch_losses = []\n",
    "        avg_kl = None\n",
    "        \n",
    "        for i in range(0, len(all_examples), BATCH_SIZE):\n",
    "            batch = all_examples[i:i+BATCH_SIZE]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch\n",
    "                boards = torch.cat([ex[0] for ex in batch]).to(device)\n",
    "                policy_targets = torch.stack([ex[1] for ex in batch]).to(device)\n",
    "                value_targets = torch.tensor([ex[2] for ex in batch], dtype=torch.float).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                policy_logits, value_preds = model(boards)\n",
    "                with torch.no_grad():\n",
    "                    ref_policy_logits, _ = reference_model(boards)\n",
    "                    ref_policy_probs = F.softmax(ref_policy_logits, dim=1)\n",
    "\n",
    "                policy_probs = F.log_softmax(policy_logits, dim=1)\n",
    "                kl_div = F.kl_div(policy_probs, ref_policy_probs, reduction='batchmean')\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, p_loss, v_loss = RL_utils.compute_loss_mcts(policy_logits, value_preds, policy_targets, value_targets, kl_div, epoch)\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Skipping batch due to NaN loss\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_kl.append(kl_div.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//BATCH_SIZE}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_kl:\n",
    "            avg_kl = np.mean(batch_kl)\n",
    "            kl_history.append(avg_kl)\n",
    "        if batch_losses:\n",
    "            avg_loss = np.mean(batch_losses)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            if avg_kl is not None:\n",
    "                print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}, avg KL={avg_kl:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}, avg KL=N/A\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: No valid batches\")\n",
    "\n",
    "    # Plot KL-divergence history\n",
    "    plt.figure()\n",
    "    plt.plot(kl_history, label=\"KL-divergence\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"KL-divergence\")\n",
    "    plt.title(\"KL-divergence vs Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4c7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_model, device = RL_utils.load_model(ref_model_path)\n",
    "# reference_model.requires_grad_(False)\n",
    "# model, _ = RL_utils.load_resnet_model()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# # Main training loop\n",
    "# for iteration in range(5):  # 5 outer iterations of self-play + training\n",
    "#     print(f\"\\n=== Iteration {iteration+1} ===\")\n",
    "    \n",
    "#     # Generate self-play games\n",
    "#     self_play_data = generate_self_play_data(model, device, positions)\n",
    "    \n",
    "#     # Train model on self-play games\n",
    "#     train_on_mcts(reference_model, model, optimizer, self_play_data, device)\n",
    "\n",
    "#     # Save model\n",
    "#     torch.save({\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#     }, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72466a52",
   "metadata": {},
   "source": [
    "# Updated Code with new style training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a399ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts import SimpleMCTS\n",
    "from chess_utils import move_to_policy_index, board_to_tensor\n",
    "from RL_utils import blend_policies, policy_index_to_move, get_game_result\n",
    "\n",
    "def generate_self_play_data_with_blending(model, reference_model, device, start_positions, num_games=100):\n",
    "    data = []\n",
    "    model.eval()\n",
    "    reference_model.eval()\n",
    "    for game_idx in range(num_games):\n",
    "        board = random.choice(start_positions)\n",
    "        mcts = SimpleMCTS(model, device, num_simulations=400)\n",
    "        game_history = []\n",
    "        move_count = 0\n",
    "        print(f\"\\n[Game {game_idx+1}/{num_games}] Starting from position (fen={board.fen()})\")\n",
    "\n",
    "        while not board.is_game_over() and move_count < MAX_GAME_MOVES:\n",
    "            total_ply = (board.fullmove_number - 1) * 2 + (0 if board.turn else 1) # total plies (black + white moves)\n",
    "            if total_ply < 20:\n",
    "                temperature = 0.3\n",
    "                alpha = 0.3 # Mostly CNN\n",
    "            elif total_ply < 50:\n",
    "                temperature = 0.5\n",
    "                alpha = 0.6\n",
    "            else:\n",
    "                temperature = 0.2\n",
    "                alpha = 1.0 # Mostly mcts\n",
    "\n",
    "            action_probs, root = mcts.get_action_probs(board, temperature)\n",
    "\n",
    "            # Fallback: if MCTS failed (should be rare), use uniform over legals\n",
    "            legal_moves = list(board.legal_moves)\n",
    "            if not action_probs or len(legal_moves) == 0:\n",
    "                mcts_move_probs = {m: 1.0 / max(1, len(legal_moves)) for m in legal_moves}\n",
    "            else:\n",
    "                # ensure we only carry probs for legal moves, in the same order\n",
    "                mcts_move_probs = {m: action_probs.get(m, 0.0) for m in legal_moves}\n",
    "\n",
    "            # Get CNN policy\n",
    "            with torch.no_grad():\n",
    "                board_tensor = board_to_tensor(board)\n",
    "                input_tensor = torch.tensor(board_tensor, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                cnn_logits, _ = reference_model(input_tensor)\n",
    "                cnn_full = torch.softmax(cnn_logits, dim=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            # Extract the CNN prob for each legal move (via index mapping)\n",
    "            cnn_move_probs = {}\n",
    "            for m in legal_moves:\n",
    "                idx = move_to_policy_index(m)\n",
    "                p = cnn_full[idx] if idx is not None else 0.0\n",
    "                cnn_move_probs[m] = float(p)\n",
    "            # Normalize CNN mass only over legal moves\n",
    "            s = sum(cnn_move_probs.values())\n",
    "            if s > 0:\n",
    "                for m in cnn_move_probs:\n",
    "                    cnn_move_probs[m] /= s\n",
    "            else:\n",
    "                # If CNN assigns zero to all legal moves (mapping/mask oddity),\n",
    "                # make it uniform over legals instead of all zeros.\n",
    "                u = 1.0 / len(legal_moves)\n",
    "                for m in cnn_move_probs:\n",
    "                    cnn_move_probs[m] = u\n",
    "\n",
    "            # --- Blend (opening only), then renormalize ---\n",
    "            blended = {m: alpha * mcts_move_probs[m] + (1 - alpha) * cnn_move_probs[m] for m in legal_moves}\n",
    "\n",
    "            # Renormalize blended (safety against numerical drift)\n",
    "            bsum = sum(blended.values())\n",
    "            if bsum <= 0 or not np.isfinite(bsum):\n",
    "                # final safety: uniform over legals\n",
    "                blended = {m: 1.0 / len(legal_moves) for m in legal_moves}\n",
    "            else:\n",
    "                for m in blended:\n",
    "                    blended[m] /= bsum\n",
    "\n",
    "            # --- Build 4288-dim training target vector from blended over legals ---\n",
    "            policy_vector = np.zeros(4288, dtype=np.float32)\n",
    "            for m, p in blended.items():\n",
    "                idx = move_to_policy_index(m)\n",
    "                if idx is not None:\n",
    "                    policy_vector[idx] = p\n",
    "\n",
    "            # --- STORE TRAINING DATA *BEFORE* MAKING THE MOVE ---\n",
    "            board_tensor = torch.tensor(board_to_tensor(board), dtype=torch.float32).unsqueeze(0)\n",
    "            current_turn = board.turn  # who is about to play now\n",
    "            game_history.append((board_tensor, policy_vector, current_turn))\n",
    "\n",
    "            # --- Sample move ONLY among legal moves using blended distribution ---\n",
    "            moves = legal_moves\n",
    "            probs = np.array([blended[m] for m in moves], dtype=np.float64)\n",
    "            # Final normalization for np.random.choice requirements\n",
    "            psum = probs.sum()\n",
    "            if psum <= 0 or not np.isfinite(psum):\n",
    "                probs[:] = 1.0 / len(probs)\n",
    "            else:\n",
    "                probs /= psum\n",
    "\n",
    "            chosen = np.random.choice(len(moves), p=probs)\n",
    "            move = moves[chosen]\n",
    "            board.push(moves[chosen])\n",
    "            move_count += 1\n",
    "\n",
    "            if move_count == 1:\n",
    "                entropy = -np.sum(probs * np.log(probs + 1e-12))\n",
    "                print(f\"   Move {move_count}: sampled {move}, \"\n",
    "                      f\"probs min={probs.min():.4f}, max={probs.max():.4f}, \"\n",
    "                      f\"entropy={entropy:.2f}\")\n",
    "            elif move_count % 20 == 0:\n",
    "                entropy = -np.sum(probs * np.log(probs + 1e-12))\n",
    "                print(f\"   Move {move_count}: sampled {move}, \"\n",
    "                      f\"probs min={probs.min():.4f}, max={probs.max():.4f}, \"\n",
    "                      f\"entropy={entropy:.2f}, fen={board.fen()}\")\n",
    "\n",
    "        # Get game result\n",
    "        result = get_game_result(board)\n",
    "        data.append((game_history, result))\n",
    "        print(f\"[Game {game_idx+1}] Finished in {move_count} moves. Result: {result}\")\n",
    "        print(f\"End position (fen={board.fen()})\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b577c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/CNN_ResNet.pth\n",
      "Initialized new model.\n",
      "\n",
      "=== Iteration 1 ===\n",
      "\n",
      "[Game 1/100] Starting from position (fen=rnbqkb1r/pp3ppp/4pn2/8/3B4/2N2N2/PPP2PPP/R2QKB1R w KQkq - 0 8)\n",
      "   Move 1: sampled f1d3, probs min=0.0009, max=0.4733, entropy=1.66\n",
      "   Move 20: sampled c7d7, probs min=0.0073, max=0.4067, entropy=2.54, fen=4rb1r/1p1qkppp/pBn1p3/7n/1P6/2N1Q2P/P1P2PP1/R4RK1 w - - 0 18\n",
      "   Move 40: sampled h8f8, probs min=0.0012, max=0.5383, entropy=1.62, fen=4rr2/1p2kp1p/pB2p1p1/6Pn/1P1n1P2/P1P4P/5R2/2Q1R1K1 w - - 1 28\n",
      "   Move 60: sampled g8a8, probs min=0.0015, max=0.4187, entropy=1.74, fen=r7/4r2p/pp1kp1p1/6Pn/1P1B1P2/P1P4P/4RR2/6K1 w - - 0 38\n",
      "   Move 80: sampled h7c7, probs min=0.0091, max=0.9176, entropy=0.39, fen=rBk5/2r5/p3p1p1/1pR3Pp/1P5P/P1P5/7K/1R2n3 w - - 4 48\n",
      "   Move 100: sampled d3b4, probs min=0.0007, max=0.5581, entropy=0.85, fen=r1k5/B7/p5p1/1p1R2Pp/1n2p2P/P1P5/6K1/8 w - - 0 58\n",
      "   Move 120: sampled a8a7, probs min=0.0018, max=0.6387, entropy=1.32, fen=1r6/k7/p5p1/6Pp/Pp2p2P/2nR4/6K1/8 w - - 0 68\n",
      "   Move 140: sampled d3d1, probs min=0.0013, max=0.7085, entropy=1.16, fen=8/k7/p5p1/P5Pp/4p2P/1p6/8/2nr2KR w - - 3 78\n",
      "   Move 160: sampled a6b5, probs min=0.0220, max=0.8565, entropy=0.60, fen=1k6/8/6p1/Pp4Pp/7P/1p2p3/7K/8 w - - 0 88\n",
      "[Game 1] Finished in 160 moves. Result: 0\n",
      "End position (fen=1k6/8/6p1/Pp4Pp/7P/1p2p3/7K/8 w - - 0 88)\n",
      "\n",
      "[Game 2/100] Starting from position (fen=r2qk2r/ppp2ppp/2p2n2/2b1p3/4P3/5Q1P/PPPP1PP1/RNB2RK1 w kq - 1 8)\n",
      "   Move 1: sampled d2d3, probs min=0.0011, max=0.7110, entropy=1.17\n",
      "   Move 20: sampled f4g3, probs min=0.0072, max=0.4162, entropy=2.49, fen=2q2Bk1/1pp2p2/r1p4p/8/p2bP3/2NP2pP/PPP2P2/4RRK1 w - - 0 18\n",
      "   Move 40: sampled b5b6, probs min=0.0006, max=0.7408, entropy=1.18, fen=6k1/b1p5/rqp2P1p/8/ppP5/1P1P2PP/P3N3/4R1RK w - - 3 28\n",
      "   Move 60: sampled f6e6, probs min=0.0003, max=0.5011, entropy=1.47, fen=5R2/2p4k/r1p1q3/6bp/ppP5/1P1P2PP/PR2N3/4R1K1 w - - 3 38\n",
      "   Move 80: sampled g7h8, probs min=0.0003, max=0.7290, entropy=1.07, fen=7k/2p5/2p5/r7/1pP3p1/pP4PP/PRq3R1/6K1 w - - 2 48\n",
      "   Move 100: sampled b5b8, probs min=0.0007, max=0.8520, entropy=0.61, fen=1r6/2p2k2/2p5/2P5/1pR3pP/pP4P1/PR6/6K1 w - - 5 58\n",
      "   Move 120: sampled d7d8, probs min=0.0007, max=0.9637, entropy=0.21, fen=3k4/2p5/r1p5/1RP5/1p5P/pP4P1/P7/6KR w - - 11 68\n",
      "   Move 140: sampled c8d8, probs min=0.0284, max=0.7912, entropy=0.69, fen=3k4/2p5/2p5/2P3RP/2P5/p5P1/P6K/7R w - - 7 78\n",
      "   Move 160: sampled f6e6, probs min=0.0036, max=0.7948, entropy=0.76, fen=8/2p5/2p1k2P/2P5/2PR4/p5P1/P1R4K/8 w - - 1 88\n",
      "[Game 2] Finished in 160 moves. Result: 0\n",
      "End position (fen=8/2p5/2p1k2P/2P5/2PR4/p5P1/P1R4K/8 w - - 1 88)\n",
      "\n",
      "[Game 3/100] Starting from position (fen=r1bq1rk1/ppp1b1pp/2n5/3pp3/8/3P1NN1/PPPB1PPP/R2QK2R w KQ - 4 10)\n",
      "   Move 1: sampled b2b3, probs min=0.0005, max=0.7040, entropy=1.10\n",
      "   Move 20: sampled e7f6, probs min=0.0031, max=0.4045, entropy=2.87, fen=rn2b1k1/ppp3pp/3q1b2/5r2/2N1p2P/1P1P4/P1PB1PP1/R2Q1RK1 w - - 4 20\n",
      "   Move 40: sampled d6d5, probs min=0.0016, max=0.3885, entropy=1.99, fen=rn6/pp4kp/6b1/3pbrQ1/1B2p3/PP1P4/2P2PP1/R4RK1 w - - 0 30\n",
      "   Move 60: sampled f6g6, probs min=0.0006, max=0.5154, entropy=1.66, fen=r7/pp1n1b1p/6kB/3p4/8/PPPp4/2R3PK/5r2 w - - 2 40\n",
      "   Move 80: sampled b7b5, probs min=0.0045, max=0.2403, entropy=2.31, fen=5nbr/8/p7/1p1p1k1p/2P4K/PP6/6P1/2R1B3 w - - 0 50\n",
      "   Move 100: sampled f5e5, probs min=0.0029, max=0.1978, entropy=2.40, fen=5n2/7b/p2r4/1P1pk2p/8/PP2B1K1/2R3P1/8 w - - 13 60\n",
      "   Move 120: sampled c4d3, probs min=0.0091, max=0.9581, entropy=0.22, fen=2r2n2/2R5/pP6/3p3p/PP6/3k4/6PK/4B3 w - - 3 70\n",
      "   Move 140: sampled a4b5, probs min=0.0075, max=0.4878, entropy=1.77, fen=8/8/pP6/Pk1p3p/8/4K1n1/6P1/8 w - - 4 80\n",
      "   Move 160: sampled c4c3, probs min=0.0020, max=0.8787, entropy=0.52, fen=1N6/8/8/p6p/3p4/2k3P1/7K/8 w - - 1 90\n",
      "[Game 3] Finished in 160 moves. Result: 0\n",
      "End position (fen=1N6/8/8/p6p/3p4/2k3P1/7K/8 w - - 1 90)\n",
      "\n",
      "[Game 4/100] Starting from position (fen=rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1)\n",
      "   Move 1: sampled g7g6, probs min=0.0013, max=0.6490, entropy=1.17\n",
      "   Move 20: sampled d2d3, probs min=0.0092, max=0.3970, entropy=2.31, fen=r2q2nr/ppp1pNbp/3p1np1/3k1b2/8/3P4/PPP2PPP/RNB2RK1 b - - 0 11\n",
      "   Move 40: sampled b1d2, probs min=0.0085, max=0.4371, entropy=2.28, fen=r1q4r/p1pn2bp/2kp2p1/4nb2/PPP5/4B3/3N1PPP/R4RK1 b - - 2 21\n",
      "   Move 60: sampled f2f3, probs min=0.0014, max=0.6325, entropy=1.40, fen=1r3b1r/pqpn4/3k2p1/P2P1b2/RP5p/5P2/1n4PP/3RN2K b - - 0 31\n",
      "   Move 80: sampled c2e2, probs min=0.0002, max=0.9381, entropy=0.39, fen=6rr/p1pnb3/3k2p1/P7/RP5p/1N3P1P/4RKP1/8 b - - 7 41\n",
      "   Move 100: sampled b3e3, probs min=0.0007, max=0.6337, entropy=1.15, fen=5nr1/p2k4/6p1/P5br/1p2K2p/4RP1P/R5P1/7N b - - 1 51\n",
      "   Move 120: sampled b7b4, probs min=0.0000, max=0.9922, entropy=0.06, fen=3r1k2/p6n/6p1/P2Rb2r/1R2KPPp/7P/8/7N b - - 0 61\n",
      "   Move 140: sampled f5e4, probs min=0.0041, max=0.9675, entropy=0.17, fen=4kr2/7n/p2b4/P1R3pP/2R1K2p/6NP/8/8 b - - 6 71\n",
      "   Move 160: sampled e3f3, probs min=0.0008, max=0.4814, entropy=1.36, fen=6R1/2b4n/p7/P5pP/2k4p/2N2K1P/8/8 b - - 1 81\n",
      "[Game 4] Finished in 160 moves. Result: 0\n",
      "End position (fen=6R1/2b4n/p7/P5pP/2k4p/2N2K1P/8/8 b - - 1 81)\n",
      "\n",
      "[Game 5/100] Starting from position (fen=r1bq1knr/pp1pb2p/5p2/2p4Q/4P3/8/PPP2PPP/RN2KB1R w KQ - 2 10)\n",
      "   Move 1: sampled b1c3, probs min=0.0012, max=0.6246, entropy=1.63\n",
      "   Move 20: sampled b6b5, probs min=0.0141, max=0.4078, entropy=2.14, fen=r2k2nr/1b1pb2p/p4p2/1ppN4/2B1PP2/P2K4/1PP3PP/3R2R1 w - - 0 20\n",
      "   Move 40: sampled f8e8, probs min=0.0013, max=0.6199, entropy=1.67, fen=r3r3/1bk4p/pN1p1p1n/1p3P2/4P3/b4K2/BPP3PP/1R4R1 w - - 4 30\n",
      "   Move 60: sampled c8c4, probs min=0.0004, max=0.7791, entropy=0.85, fen=rk4B1/7p/3p1p2/pp3n2/2r2KP1/b6b/1PP5/R6R w - - 0 40\n",
      "   Move 80: sampled b8c8, probs min=0.0007, max=0.4335, entropy=1.58, fen=2k5/2r4p/3p3n/1p1b1pP1/p4KB1/P1P4R/8/7R w - - 5 50\n",
      "   Move 100: sampled c4b4, probs min=0.0008, max=0.5945, entropy=1.52, fen=3k4/8/3p4/1p3bP1/pr6/P1P1K3/7R/7R w - - 4 60\n",
      "   Move 120: sampled d6d7, probs min=0.0002, max=0.9711, entropy=0.18, fen=8/3k4/8/1P4P1/p7/P2bK3/6R1/8 w - - 3 70\n",
      "   Move 140: sampled c6d7, probs min=0.0871, max=0.7247, entropy=0.76, fen=8/3k4/8/4K1P1/p7/P7/1R6/8 w - - 13 80\n",
      "   Move 160: sampled g8f8, probs min=0.0900, max=0.7708, entropy=0.69, fen=5k2/8/3K2P1/4R3/p7/P7/8/8 w - - 1 90\n",
      "[Game 5] Finished in 160 moves. Result: 0\n",
      "End position (fen=5k2/8/3K2P1/4R3/p7/P7/8/8 w - - 1 90)\n",
      "\n",
      "[Game 6/100] Starting from position (fen=rnbqkbnr/ppp2ppp/4p3/3p4/3P4/2N5/PPP1PPPP/R1BQKBNR w KQkq - 0 3)\n",
      "   Move 1: sampled g1f3, probs min=0.0014, max=0.8348, entropy=0.86\n",
      "   Move 20: sampled e7h4, probs min=0.0072, max=0.4247, entropy=2.72, fen=r1bqk2r/1p3p1p/p1n1p3/3p4/3P1pnb/2NQ4/PPP1BPPP/R4RK1 w kq - 0 13\n",
      "   Move 40: sampled a8b8, probs min=0.0067, max=0.4084, entropy=2.45, fen=1rbk3r/q6p/1p1Npp2/1p6/3P2nb/5p2/PPP2P1P/1R3RK1 w - - 4 23\n",
      "   Move 60: sampled e2d1, probs min=0.0004, max=0.6178, entropy=1.48, fen=qr1k1r2/7p/1p1N1p2/3p4/1p4P1/1PP2p2/P4b1K/3b1R2 w - - 0 33\n",
      "   Move 80: sampled e8d7, probs min=0.0011, max=0.4099, entropy=2.17, fen=q4r2/3k3r/1p5N/3p4/1P4p1/1P3pK1/P7/R7 w - - 3 43\n",
      "   Move 100: sampled h6h3, probs min=0.0006, max=0.5879, entropy=1.50, fen=q2k4/8/1p6/3pK3/PP6/1P3p1r/8/3R4 w - - 3 53\n",
      "   Move 120: sampled b4h4, probs min=0.0002, max=0.5666, entropy=1.15, fen=8/8/1P2k3/2Kp2q1/7r/1P3p2/8/2R5 w - - 3 63\n",
      "   Move 140: sampled h4h6, probs min=0.0011, max=0.4822, entropy=1.42, fen=8/P3R3/K6r/5k2/3p4/1P6/5p2/8 w - - 2 73\n",
      "   Move 160: sampled g4e2, probs min=0.0017, max=0.4202, entropy=1.62, fen=8/P1K5/8/2R5/1P2k2r/3p4/4b3/8 w - - 2 83\n",
      "[Game 6] Finished in 160 moves. Result: 0\n",
      "End position (fen=8/P1K5/8/2R5/1P2k2r/3p4/4b3/8 w - - 2 83)\n",
      "\n",
      "[Game 7/100] Starting from position (fen=rnbqkbnr/pp2pppp/8/3pP3/3p4/8/PPP2PPP/RNBQKBNR w KQkq - 0 4)\n",
      "   Move 1: sampled e1e2, probs min=0.0013, max=0.7012, entropy=1.51\n",
      "   Move 20: sampled d8e7, probs min=0.0450, max=0.7905, entropy=0.80, fen=rn4nr/pp2kppp/4p3/1N2P3/3Q2P1/8/PPP2PbP/R1B1KBNR w - - 3 14\n",
      "   Move 40: sampled d8d7, probs min=0.0065, max=0.4065, entropy=2.41, fen=r6r/pp1k1pp1/4p3/n2nP2p/P5P1/7N/NPPK1P1P/R1B2BRb w - - 5 24\n",
      "   Move 60: sampled h6h8, probs min=0.0001, max=0.9048, entropy=0.50, fen=r6r/pp2k3/4pp2/1B2P1p1/P1n1KPPp/2Nn3N/1P4RP/R1B4b w - - 2 34\n",
      "   Move 80: sampled g6h6, probs min=0.0191, max=0.9608, entropy=0.19, fen=r2r4/8/p3pP1k/Pp1BN3/4KPPp/7b/NP5P/5R2 w - - 2 44\n",
      "   Move 100: sampled e6e1, probs min=0.0000, max=0.9953, entropy=0.04, fen=8/7k/p4P2/Pp6/1N3PPp/1P3K2/2rR3P/4r3 w - - 6 54\n",
      "   Move 120: sampled f3c3, probs min=0.0061, max=0.7100, entropy=1.03, fen=8/8/p4k2/Pp6/1N1KRPP1/1Pr4p/7P/8 w - - 2 64\n",
      "   Move 140: sampled h7g8, probs min=0.0021, max=0.7455, entropy=1.04, fen=6k1/8/p7/Pp4P1/3NKP2/1P5p/5r1P/8 w - - 12 74\n",
      "   Move 160: sampled b8a8, probs min=0.0245, max=0.3149, entropy=1.52, fen=k7/8/p7/Pp2N1P1/5P2/1P1K3p/7P/8 w - - 9 84\n",
      "[Game 7] Finished in 160 moves. Result: 0\n",
      "End position (fen=k7/8/p7/Pp2N1P1/5P2/1P1K3p/7P/8 w - - 9 84)\n",
      "\n",
      "[Game 8/100] Starting from position (fen=r1bqk2r/pppp1ppp/5n2/4bP2/3NP3/2P5/PP4PP/RN1QKB1R w KQkq - 1 10)\n",
      "   Move 1: sampled b1d2, probs min=0.0008, max=0.7019, entropy=1.26\n",
      "   Move 20: sampled h7g6, probs min=0.0073, max=0.3066, entropy=2.61, fen=r1bq4/pp1p2r1/3k2p1/2p5/3N2P1/2P5/PP1N2P1/R3KB1R w KQ - 0 20\n"
     ]
    }
   ],
   "source": [
    "reference_model, device = RL_utils.load_resnet_model(ref_model_path)\n",
    "reference_model.requires_grad_(False)\n",
    "reference_model.eval()\n",
    "model, _ = RL_utils.load_resnet_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\n=== Iteration {iteration+1} ===\")\n",
    "    # Generate self-play games with policy blending\n",
    "    self_play_data = generate_self_play_data_with_blending(model, reference_model, device, positions, num_games=NUM_SELF_PLAY_GAMES)\n",
    "    # Train model on self-play games (with KL loss for opening positions)\n",
    "    train_on_mcts(reference_model, model, optimizer, self_play_data, device)\n",
    "    save_name = MODEL_SAVE_PATH + \"_\" + iteration + \".pth\"\n",
    "    try:\n",
    "        torch.save({'model_state_dict': model.state_dict()}, save_name)\n",
    "    except:\n",
    "        torch.save({'model_state_dict': model.state_dict()}, \"rl_backup_save.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfca9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
