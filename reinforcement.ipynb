{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e16c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import RL_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad67955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our existing CNN model\n",
    "ref_model_path = \"models/CNN_ResNet.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bfe58",
   "metadata": {},
   "source": [
    "## Load stockfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3143cc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PositionEvaluator' from 'RL_utils' (/home/smith3_j@WMGDS.WMG.WARWICK.AC.UK/Documents/jordi-lete/Chess-AI/RL_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRL_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositionEvaluator\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PositionEvaluator' from 'RL_utils' (/home/smith3_j@WMGDS.WMG.WARWICK.AC.UK/Documents/jordi-lete/Chess-AI/RL_utils.py)"
     ]
    }
   ],
   "source": [
    "from RL_utils import PositionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df9168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stockfish initialized with ELO: 1400\n"
     ]
    }
   ],
   "source": [
    "stockfish_path = \"models/stockfish/stockfish-windows-x86-64-avx2.exe\"\n",
    "evaluator = PositionEvaluator(stockfish_path, elo_rating=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b4ef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Move.from_uci('h7h6')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = chess.Board()\n",
    "board.push_uci(\"e2e4\")\n",
    "board.push_uci(\"h7h6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8d0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position evaluation: 0.83\n"
     ]
    }
   ],
   "source": [
    "eval_score = evaluator.evaluate_position(board)\n",
    "print(f\"Position evaluation: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364ac607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best moves: [{'Move': 'd2d4', 'Centipawn': 83, 'Mate': None}, {'Move': 'g1f3', 'Centipawn': 71, 'Mate': None}, {'Move': 'b1c3', 'Centipawn': 68, 'Mate': None}]\n"
     ]
    }
   ],
   "source": [
    "best_moves = evaluator.get_best_moves(board, 3)\n",
    "print(f\"Best moves: {best_moves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f434f5",
   "metadata": {},
   "source": [
    "## Load our CNN trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46049a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_model, device = RL_utils.load_model(ref_model_path)\n",
    "# reference_model = reference_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605a6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# dummy_input = torch.randn(1, 19, 8, 8).to(device)\n",
    "# with torch.no_grad():\n",
    "#     output = model(dummy_input)\n",
    "#     print(f\"Model output shape: {output.shape}\")  # Should be [1, 4288]\n",
    "#     print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7e154",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2570efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1000 varied positions (openings, early, middlegame)\n",
      "Total positions loaded: 1000\n"
     ]
    }
   ],
   "source": [
    "pgn_file = \"games/lichess_db_2016-04.pgn\"\n",
    "positions = []\n",
    "positions += RL_utils.extract_varied_positions(pgn_file, num_positions=1000)\n",
    "print(f\"Total positions loaded: {len(positions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ea359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creating a batch\n",
    "# if positions:\n",
    "#     board_tensors, legal_masks, boards = RL_utils.create_training_batch(positions, batch_size=4)\n",
    "#     print(f\"Batch shapes:\")\n",
    "#     print(f\"  Board tensors: {board_tensors.shape}\")\n",
    "#     print(f\"  Legal masks: {legal_masks.shape}\")\n",
    "#     print(f\"  Number of boards: {len(boards)}\")\n",
    "    \n",
    "#     # Show a sample position\n",
    "#     print(f\"\\nSample position FEN: {boards[0].fen()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3324c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boards[0]\n",
    "# eval_score = evaluator.evaluate_position(boards[0])\n",
    "# print(f\"Position evaluation: {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5400975",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1ab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976b0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "\n",
    "# Self-play\n",
    "NUM_SELF_PLAY_GAMES = 100\n",
    "MAX_GAME_MOVES = 512\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "# Model Saving\n",
    "MODEL_SAVE_PATH = \"models/rlmodel_buffer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92ff69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_self_play_data(model, device, start_positions, num_games=NUM_SELF_PLAY_GAMES):\n",
    "    \"\"\"Generate self-play data with error handling\"\"\"\n",
    "    data = []\n",
    "    successful_games = 0\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        try:\n",
    "            board = random.choice(start_positions)\n",
    "            # game_history, result = RL_utils.play_self_play_game(model, device, board, MAX_GAME_MOVES, TEMPERATURE)\n",
    "            game_history, result = RL_utils.play_game_with_mcts(model, device, board, MAX_GAME_MOVES, TEMPERATURE)\n",
    "            \n",
    "            if len(game_history) > 0:  # Only add if we have valid data\n",
    "                data.append((game_history, result))\n",
    "                successful_games += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in game {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Generated {successful_games} successful games out of {num_games} attempts\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83df7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_self_play(model, optimizer, game_histories, device):\n",
    "    \"\"\"Train model on self-play data with gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Flatten all examples from all games\n",
    "    all_examples = []\n",
    "    for history, result in game_histories:\n",
    "        for board_tensor, legal_mask, move_idx, turn in history:\n",
    "            # Convert result to value from current player's perspective\n",
    "            value = result if turn else -result\n",
    "            all_examples.append((board_tensor, legal_mask, move_idx, value))\n",
    "    \n",
    "    if len(all_examples) == 0:\n",
    "        print(\"No training examples available!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Training on {len(all_examples)} examples\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        random.shuffle(all_examples)\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for i in range(0, len(all_examples), BATCH_SIZE):\n",
    "            batch = all_examples[i:i+BATCH_SIZE]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch\n",
    "                boards = torch.cat([ex[0] for ex in batch]).to(device)\n",
    "                masks = torch.stack([ex[1] for ex in batch]).to(device)\n",
    "                move_targets = torch.tensor([ex[2] for ex in batch], dtype=torch.long).to(device)\n",
    "                value_targets = torch.tensor([ex[3] for ex in batch], dtype=torch.float).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                policy_logits, value_preds = model(boards)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, p_loss, v_loss = RL_utils.compute_loss(policy_logits, value_preds, move_targets, value_targets, masks)\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Skipping batch due to NaN loss\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                epoch_losses.append(loss.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//BATCH_SIZE}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if epoch_losses:\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: No valid batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f5f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_on_mcts(reference_model, model, optimizer, game_histories, device):\n",
    "    \"\"\"Train model on self-play data with gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Flatten all examples from all games\n",
    "    all_examples = []\n",
    "    for history, result in game_histories:\n",
    "        for board_tensor, policy_vector, turn in history:\n",
    "            # Convert result to value from current player's perspective\n",
    "            value = result if turn else -result\n",
    "            policy_tensor = torch.tensor(policy_vector, dtype=torch.float32)\n",
    "            all_examples.append((board_tensor, policy_tensor, value))\n",
    "    \n",
    "    if len(all_examples) == 0:\n",
    "        print(\"No training examples available!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Training on {len(all_examples)} examples\")\n",
    "    \n",
    "    kl_history = []\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        random.shuffle(all_examples)\n",
    "        batch_kl = []\n",
    "        batch_losses = []\n",
    "        avg_kl = None\n",
    "        \n",
    "        for i in range(0, len(all_examples), BATCH_SIZE):\n",
    "            batch = all_examples[i:i+BATCH_SIZE]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch\n",
    "                boards = torch.cat([ex[0] for ex in batch]).to(device)\n",
    "                policy_targets = torch.stack([ex[1] for ex in batch]).to(device)\n",
    "                value_targets = torch.tensor([ex[2] for ex in batch], dtype=torch.float).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                policy_logits, value_preds = model(boards)\n",
    "                with torch.no_grad():\n",
    "                    ref_policy_logits, _ = reference_model(boards)\n",
    "                    ref_policy_probs = F.softmax(ref_policy_logits, dim=1)\n",
    "\n",
    "                policy_probs = F.log_softmax(policy_logits, dim=1)\n",
    "                kl_div = F.kl_div(policy_probs, ref_policy_probs, reduction='batchmean')\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, p_loss, v_loss = RL_utils.compute_loss_mcts(policy_logits, value_preds, policy_targets, value_targets, kl_div, epoch)\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Skipping batch due to NaN loss\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_kl.append(kl_div.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//BATCH_SIZE}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_kl:\n",
    "            avg_kl = np.mean(batch_kl)\n",
    "            kl_history.append(avg_kl)\n",
    "        if batch_losses:\n",
    "            avg_loss = np.mean(batch_losses)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            if avg_kl is not None:\n",
    "                print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}, avg KL={avg_kl:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}, avg KL=N/A\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: No valid batches\")\n",
    "\n",
    "    # Plot KL-divergence history\n",
    "    plt.figure()\n",
    "    plt.plot(kl_history, label=\"KL-divergence\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"KL-divergence\")\n",
    "    plt.title(\"KL-divergence vs Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4400115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_utils import compute_loss_mcts\n",
    "\n",
    "def train_on_buffer(model, reference_model, optimizer, replay_buffer, device,\n",
    "                    examples_per_epoch=8000, epochs=2, batch_size=64,\n",
    "                    kl_schedule_epoch_offset=0):\n",
    "    \"\"\"\n",
    "    Train model using examples sampled uniformly from replay_buffer.\n",
    "    - examples_per_epoch: how many training examples to sample (flattened) per epoch\n",
    "    - epochs: how many epochs to run over freshly sampled examples each call\n",
    "    - kl_schedule_epoch_offset: if you use epoch-dependent KL beta, pass offset for compute_loss\n",
    "    \"\"\"\n",
    "    if len(replay_buffer) == 0:\n",
    "        print(\"Replay buffer empty - skipping training\")\n",
    "        return\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # sample examples_for_epoch fresh each epoch (no repeat guarantee)\n",
    "        examples = replay_buffer.sample_examples(examples_per_epoch)\n",
    "        if len(examples) == 0:\n",
    "            print(\"No examples sampled - skipping epoch\")\n",
    "            continue\n",
    "\n",
    "        # shuffle examples\n",
    "        random.shuffle(examples)\n",
    "        epoch_losses = []\n",
    "        policy_loss_vals = []\n",
    "        value_loss_vals = []\n",
    "        kl_vals = []\n",
    "        kl_contrib_vals = []\n",
    "        for i in range(0, len(examples), batch_size):\n",
    "            batch = examples[i:i+batch_size]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "\n",
    "            # Prepare batch\n",
    "            boards = torch.cat([ex[0] for ex in batch]).to(device)           # shape [B, C, H, W]\n",
    "            policy_targets = torch.stack([torch.tensor(ex[1], dtype=torch.float32) for ex in batch]).to(device)\n",
    "            value_targets = torch.tensor([ex[2] for ex in batch], dtype=torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            policy_logits, value_preds = model(boards)\n",
    "\n",
    "            # reference policy for KL (detached)\n",
    "            with torch.no_grad():\n",
    "                ref_policy_logits, _ = reference_model(boards)\n",
    "                ref_policy_probs = F.softmax(ref_policy_logits, dim=1)\n",
    "\n",
    "            # KL direction: KL(model || ref) means expectation under model of log(model/ref)\n",
    "            # using PyTorch F.kl_div which expects log-probs and targets probs: kl_div(log_p, q)\n",
    "            policy_log_probs = F.log_softmax(policy_logits, dim=1)\n",
    "            kl_div = F.kl_div(policy_log_probs, ref_policy_probs, reduction='batchmean')\n",
    "\n",
    "            # compute loss - assumes compute_loss_mcts has signature:\n",
    "            # compute_loss_mcts(policy_logits, value_preds, policy_targets, value_targets, kl_div, epoch_index)\n",
    "            loss, p_loss, v_loss, kl_beta, kl_div_tensor = compute_loss_mcts(policy_logits, value_preds, policy_targets, value_targets, kl_div, epoch + kl_schedule_epoch_offset)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(\"Skipping batch due to NaN/Inf loss\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "            policy_loss_vals.append(p_loss.item()) # should meaningfully decrease during training if the model is matching the blended MCTS/CNN soft targets.\n",
    "            value_loss_vals.append(v_loss.item()) # tells us whether value prediction is improving. If it is small relative to policy, the network is focusing on policy.\n",
    "            kl_vals.append(kl_div_tensor.item()) # tells us the divergence between ref and model.\n",
    "            kl_contrib_vals.append(kl_beta * kl_div_tensor.item()) # avg KL*beta is how much it contributes to total_loss. If avg KL*beta is dominating, the model will be strongly forced to remain near the CNN.\n",
    "\n",
    "        if epoch_losses:\n",
    "            print(f\"Train epoch {epoch+1}/{epochs}: avg total loss={np.mean(epoch_losses):.4f}, \"\n",
    "                  f\"policy={np.mean(policy_loss_vals):.4f}, value={np.mean(value_loss_vals):.4f}, \"\n",
    "                  f\"avg KL={np.mean(kl_vals):.4f}, avg KL*beta={np.mean(kl_contrib_vals):.4f}, \"\n",
    "                  f\"examples={len(examples)}\")\n",
    "        else:\n",
    "            print(f\"Train epoch {epoch+1}/{epochs}: no valid batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4c7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_model, device = RL_utils.load_model(ref_model_path)\n",
    "# reference_model.requires_grad_(False)\n",
    "# model, _ = RL_utils.load_resnet_model()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# # Main training loop\n",
    "# for iteration in range(5):  # 5 outer iterations of self-play + training\n",
    "#     print(f\"\\n=== Iteration {iteration+1} ===\")\n",
    "    \n",
    "#     # Generate self-play games\n",
    "#     self_play_data = generate_self_play_data(model, device, positions)\n",
    "    \n",
    "#     # Train model on self-play games\n",
    "#     train_on_mcts(reference_model, model, optimizer, self_play_data, device)\n",
    "\n",
    "#     # Save model\n",
    "#     torch.save({\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#     }, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72466a52",
   "metadata": {},
   "source": [
    "# Updated Code with new style training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a399ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts import SimpleMCTS\n",
    "from chess_utils import move_to_policy_index, board_to_tensor\n",
    "from RL_utils import policy_index_to_move, get_game_result\n",
    "\n",
    "def generate_self_play_data_with_blending(\n",
    "    model,\n",
    "    reference_model,\n",
    "    device,\n",
    "    start_positions,\n",
    "    num_games=100,\n",
    "    num_simulations=400,\n",
    "    max_game_moves=200,\n",
    "    # blending / temperature schedule params\n",
    "    opening_ply_cutoff=20,\n",
    "    midgame_ply_cutoff=50,\n",
    "    alpha_opening=0.2,\n",
    "    alpha_mid=0.6,\n",
    "    # resignation params (safe defaults)\n",
    "    resign_threshold=-0.8,    # v_curr <= this is considered very losing\n",
    "    resign_min_plies=20,       # don't resign before this many plies\n",
    "    resign_consec=4            # require this many consecutive low-value preds\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate self-play games with blended MCTS + CNN policies and resign logic.\n",
    "    Returns list of (game_history, result) where game_history = [(board_tensor, policy_vector, turn), ...]\n",
    "    Result is from WHITE's POV: +1 white win, -1 black win, 0 draw (or other fallback from get_game_result).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    reference_model.eval()\n",
    "    data = []\n",
    "\n",
    "    resign_count = 0\n",
    "\n",
    "    for game_idx in range(num_games):\n",
    "        # copy starting position so we don't mutate the pool\n",
    "        board = random.choice(start_positions).copy()\n",
    "        mcts = SimpleMCTS(model, device, num_simulations=num_simulations)\n",
    "        game_history = []\n",
    "        move_count = 0\n",
    "        consec_low = 0\n",
    "        early_ended = False\n",
    "\n",
    "        print(f\"\\n[Game {game_idx+1}/{num_games}] Starting from position (fen={board.fen()})\")\n",
    "\n",
    "        while not board.is_game_over() and move_count < max_game_moves:\n",
    "            total_ply = (board.fullmove_number - 1) * 2 + (0 if board.turn == chess.WHITE else 1)\n",
    "\n",
    "            # --- Evaluate value head for resignation decision (use current RL model) ---\n",
    "            with torch.no_grad():\n",
    "                bt = torch.tensor(board_to_tensor(board), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                _, v_white = model(bt)                     # shape [1,1]\n",
    "                v_white = float(v_white.item())           # in [-1,1], white's POV\n",
    "\n",
    "            # current-player perspective: +1 = good for player to move\n",
    "            v_curr = v_white if board.turn == chess.WHITE else -v_white\n",
    "\n",
    "            # consecutive low-value check\n",
    "            if total_ply >= resign_min_plies and v_curr <= resign_threshold:\n",
    "                consec_low += 1\n",
    "            else:\n",
    "                consec_low = 0\n",
    "\n",
    "            if consec_low >= resign_consec:\n",
    "                # current player resigns -> opponent wins\n",
    "                result = -1.0 if board.turn == chess.WHITE else 1.0  # white POV\n",
    "                resign_count += 1\n",
    "                print(f\"[Game {game_idx+1}] Resignation by {'White' if board.turn==chess.WHITE else 'Black'} \"\n",
    "                      f\"at ply {total_ply}, v_curr={v_curr:.3f}, consec_low={consec_low}\")\n",
    "                data.append((game_history, result))\n",
    "                early_ended = True\n",
    "                break\n",
    "\n",
    "            # legal moves (quick check)\n",
    "            legal_moves = list(board.legal_moves)\n",
    "            if len(legal_moves) == 0:\n",
    "                break  # terminal (shouldn't happen normally)\n",
    "\n",
    "            # --- CNN (reference) policy for legal moves (compute FIRST so we can adapt blending/temperature) ---\n",
    "            with torch.no_grad():\n",
    "                bt_ref = torch.tensor(board_to_tensor(board), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                cnn_logits, _ = reference_model(bt_ref)\n",
    "                cnn_full = torch.softmax(cnn_logits, dim=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            # Extract CNN probs for legal moves\n",
    "            cnn_move_probs = {}\n",
    "            for m in legal_moves:\n",
    "                idx = move_to_policy_index(m)\n",
    "                p = float(cnn_full[idx]) if idx is not None else 0.0\n",
    "                cnn_move_probs[m] = p\n",
    "\n",
    "            # Normalize CNN mass only over legal moves (safety)\n",
    "            s = sum(cnn_move_probs.values())\n",
    "            if s > 0:\n",
    "                for m in cnn_move_probs:\n",
    "                    cnn_move_probs[m] /= s\n",
    "            else:\n",
    "                u = 1.0 / len(legal_moves)\n",
    "                for m in cnn_move_probs:\n",
    "                    cnn_move_probs[m] = u\n",
    "\n",
    "            # --- Compute CNN uncertainty (entropy) over legal moves ---\n",
    "            eps = 1e-12\n",
    "            legal_probs = np.array([cnn_move_probs[m] for m in legal_moves], dtype=np.float64)\n",
    "            lp_sum = legal_probs.sum()\n",
    "            if lp_sum > 0:\n",
    "                legal_probs /= lp_sum\n",
    "            else:\n",
    "                legal_probs[:] = 1.0 / len(legal_probs)\n",
    "\n",
    "            entropy = -np.sum(legal_probs * np.log(legal_probs + eps))\n",
    "            max_entropy = np.log(len(legal_probs)) if len(legal_probs) > 0 else 1.0\n",
    "            norm_entropy = float(entropy / max_entropy) if max_entropy > 0 else 1.0  # 0=confident,1=uncertain\n",
    "            cnn_pmax = float(legal_probs.max())\n",
    "\n",
    "            # --- schedule base_alpha and base_temperature from ply (your existing schedule) ---\n",
    "            if total_ply < opening_ply_cutoff:\n",
    "                base_temperature = 0.8\n",
    "                base_alpha = alpha_opening\n",
    "            elif total_ply < midgame_ply_cutoff:\n",
    "                base_temperature = 0.7\n",
    "                base_alpha = alpha_mid\n",
    "            else:\n",
    "                base_temperature = 0.2\n",
    "                base_alpha = 1.0\n",
    "\n",
    "            # --- adapt alpha: move from base_alpha toward 1.0 proportional to CNN uncertainty\n",
    "            alpha = base_alpha + norm_entropy * (1.0 - base_alpha)\n",
    "            alpha = float(np.clip(alpha, 0.0, 1.0))\n",
    "\n",
    "            # --- adapt temperature: increase when CNN uncertain (so we explore more) ---\n",
    "            temp_max = 1.0\n",
    "            temperature = float(base_temperature + norm_entropy * (temp_max - base_temperature))\n",
    "            temperature = max(0.01, min(temperature, temp_max))\n",
    "\n",
    "            pure_cnn_pmax_thresh   = 0.90\n",
    "            pure_cnn_entropy_thresh = 0.06   # normalized entropy threshold (0..1)\n",
    "            pure_cnn_value_agree_thresh = -0.6  # require model value not strongly negative\n",
    "\n",
    "            # compute whether we can trust CNN-only (opening, confident, and low entropy)\n",
    "            candidate_pure_cnn = (cnn_pmax >= pure_cnn_pmax_thresh and norm_entropy <= pure_cnn_entropy_thresh and total_ply < opening_ply_cutoff)\n",
    "\n",
    "            v_curr_rl = v_white if board.turn == chess.WHITE else -v_white\n",
    "            model_agrees = (v_curr_rl >= pure_cnn_value_agree_thresh)\n",
    "\n",
    "            # if CNN very confident in opening, use CNN-only policy\n",
    "            use_pure_cnn = candidate_pure_cnn and model_agrees\n",
    "\n",
    "            # --- Now run MCTS using the (possibly adapted) temperature when converting visits -> probs ---\n",
    "            action_probs, root = mcts.get_action_probs(board, temperature)\n",
    "\n",
    "            # Fallback MCTS uniform if needed\n",
    "            if not action_probs:\n",
    "                mcts_move_probs = {m: 1.0 / len(legal_moves) for m in legal_moves}\n",
    "            else:\n",
    "                mcts_move_probs = {m: action_probs.get(m, 0.0) for m in legal_moves}\n",
    "\n",
    "            # --- Blend (cnn vs mcts) using adaptive alpha (or pure CNN if flagged) ---\n",
    "            if use_pure_cnn:\n",
    "                blended = {m: cnn_move_probs[m] for m in legal_moves}\n",
    "            else:\n",
    "                blended = {m: alpha * mcts_move_probs.get(m, 0.0) + (1 - alpha) * cnn_move_probs.get(m, 0.0)\n",
    "                           for m in legal_moves}\n",
    "\n",
    "            # Renormalize blended (safety)\n",
    "            bsum = sum(blended.values())\n",
    "            if bsum <= 0 or not np.isfinite(bsum):\n",
    "                blended = {m: 1.0 / len(legal_moves) for m in legal_moves}\n",
    "            else:\n",
    "                for m in blended:\n",
    "                    blended[m] /= bsum\n",
    "\n",
    "            # Build policy vector (4288-dim)\n",
    "            policy_vector = np.zeros(4288, dtype=np.float32)\n",
    "            for m, p in blended.items():\n",
    "                idx = move_to_policy_index(m)\n",
    "                if idx is not None:\n",
    "                    policy_vector[idx] = p\n",
    "\n",
    "            # STORE training example (board BEFORE move)\n",
    "            board_tensor = torch.tensor(board_to_tensor(board), dtype=torch.float32).unsqueeze(0)  # CPU ok\n",
    "            current_turn = board.turn\n",
    "            game_history.append((board_tensor, policy_vector, current_turn))\n",
    "\n",
    "            # Sample and make the move among legal moves\n",
    "            moves = legal_moves\n",
    "            probs = np.array([blended[m] for m in moves], dtype=np.float64)\n",
    "            psum = probs.sum()\n",
    "            if psum <= 0 or not np.isfinite(psum):\n",
    "                probs[:] = 1.0 / len(probs)\n",
    "            else:\n",
    "                probs /= psum\n",
    "\n",
    "            chosen_idx = np.random.choice(len(moves), p=probs)\n",
    "            chosen_move = moves[chosen_idx]\n",
    "            board.push(chosen_move)\n",
    "            move_count += 1\n",
    "\n",
    "            # periodic logging (now includes entropy/alpha/temp)\n",
    "            if move_count == 1 or move_count % 20 == 0:\n",
    "                entropy_val = -np.sum(probs * np.log(probs + 1e-12))\n",
    "                print(f\"   Move {move_count}: sampled {chosen_move}, \"\n",
    "                      f\"probs min={probs.min():.4f}, max={probs.max():.4f}, entropy={entropy_val:.2f}, fen={board.fen()}\")\n",
    "                print(f\"      cnn_pmax={cnn_pmax:.3f}, norm_entropy={norm_entropy:.3f}, alpha={alpha:.3f}, temperature={temperature:.3f}, use_pure_cnn={use_pure_cnn}\")\n",
    "\n",
    "        # only append final result if we didn't already append due to early resignation\n",
    "        if not early_ended:\n",
    "            result = get_game_result(board)\n",
    "            data.append((game_history, result))\n",
    "            print(f\"[Game {game_idx+1}] Finished in {move_count} moves. Result: {result}\")\n",
    "            print(f\"End position (fen={board.fen()})\")\n",
    "\n",
    "    print(f\"Total resignations in this batch: {resign_count}/{num_games}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b577c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/CNN_ResNet.pth\n",
      "Model loaded from models/CNN_ResNet.pth\n",
      "\n",
      "=== Iteration 1 ===\n",
      "\n",
      "[Game 1/100] Starting from position (fen=rnbqk1nr/pp3ppp/4p3/2b5/4p3/8/PPPN1PPP/RNBQKB1R w KQkq - 0 6)\n",
      "   Move 1: sampled d2e4, probs min=0.0000, max=1.0000, entropy=0.00, fen=rnbqk1nr/pp3ppp/4p3/2b5/4N3/8/PPP2PPP/RNBQKB1R b KQkq - 0 6\n",
      "      cnn_pmax=1.000, norm_entropy=0.000, alpha=0.200, temperature=0.800, use_pure_cnn=True\n",
      "   Move 20: sampled b7b5, probs min=0.0002, max=0.8295, entropy=0.68, fen=r1b2rk1/4nppp/4p3/1pN2n2/2p5/2P5/PP2KPPP/R1B4R w - - 0 16\n",
      "      cnn_pmax=0.924, norm_entropy=0.081, alpha=0.632, temperature=0.724, use_pure_cnn=False\n",
      "   Move 40: sampled b5c4, probs min=0.0002, max=0.6931, entropy=1.04, fen=5rk1/1b3npp/8/P2npp2/2p5/2P3B1/R4PPP/3K3R w - - 0 26\n",
      "      cnn_pmax=1.000, norm_entropy=0.000, alpha=0.600, temperature=0.700, use_pure_cnn=False\n",
      "   Move 60: sampled e8e7, probs min=0.0000, max=0.9990, entropy=0.01, fen=6k1/4r1p1/P7/7p/2p5/5P2/K4P1P/2R5 w - - 0 36\n",
      "      cnn_pmax=0.936, norm_entropy=0.092, alpha=1.000, temperature=0.274, use_pure_cnn=False\n",
      "   Move 80: sampled d6c5, probs min=0.0000, max=0.9999, entropy=0.00, fen=8/8/6p1/2k4p/2p5/4PPR1/K6P/6Q1 w - - 4 46\n",
      "      cnn_pmax=1.000, norm_entropy=0.000, alpha=1.000, temperature=0.200, use_pure_cnn=False\n"
     ]
    }
   ],
   "source": [
    "reference_model, device = RL_utils.load_resnet_model(ref_model_path)\n",
    "reference_model.requires_grad_(False)\n",
    "reference_model.eval()\n",
    "model, _ = RL_utils.load_resnet_model(ref_model_path)\n",
    "\n",
    "# if ref_model_path:\n",
    "#     ckpt = torch.load(ref_model_path, map_location=device)\n",
    "#     model.load_state_dict(ckpt['model_state_dict'])   # warm-start weights\n",
    "#     print(\"RL model initialized from CNN checkpoint\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=12000)\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\n=== Iteration {iteration+1} ===\")\n",
    "    # Generate self-play games with policy blending\n",
    "    self_play_data = generate_self_play_data_with_blending(model, reference_model, device, positions, num_games=NUM_SELF_PLAY_GAMES)\n",
    "    replay_buffer.add_games(self_play_data)\n",
    "    # train using samples from buffer\n",
    "    train_on_buffer(model, reference_model, optimizer, replay_buffer,\n",
    "                    device,\n",
    "                    examples_per_epoch=8000,\n",
    "                    epochs=2,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "    # Train model on self-play games (with KL loss for opening positions)\n",
    "    # train_on_mcts(reference_model, model, optimizer, self_play_data, device)\n",
    "    try:\n",
    "        save_name = MODEL_SAVE_PATH + \"_\" + str(iteration) + \".pth\"\n",
    "        torch.save({'model_state_dict': model.state_dict()}, save_name)\n",
    "    except:\n",
    "        torch.save({'model_state_dict': model.state_dict()}, \"rl_backup_save.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0ce48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
