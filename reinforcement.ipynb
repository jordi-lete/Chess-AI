{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e16c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import RL_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad67955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our existing CNN model\n",
    "ref_model_path = \"models/TORCH_250EPOCH_DoubleHead.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bfe58",
   "metadata": {},
   "source": [
    "## Load stockfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3143cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_utils import PositionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df9168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stockfish initialized with ELO: 1400\n"
     ]
    }
   ],
   "source": [
    "stockfish_path = \"models/stockfish/stockfish-windows-x86-64-avx2.exe\"\n",
    "evaluator = PositionEvaluator(stockfish_path, elo_rating=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b4ef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Move.from_uci('h7h6')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = chess.Board()\n",
    "board.push_uci(\"e2e4\")\n",
    "board.push_uci(\"h7h6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8d0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position evaluation: 0.83\n"
     ]
    }
   ],
   "source": [
    "eval_score = evaluator.evaluate_position(board)\n",
    "print(f\"Position evaluation: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364ac607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best moves: [{'Move': 'd2d4', 'Centipawn': 83, 'Mate': None}, {'Move': 'g1f3', 'Centipawn': 71, 'Mate': None}, {'Move': 'b1c3', 'Centipawn': 68, 'Mate': None}]\n"
     ]
    }
   ],
   "source": [
    "best_moves = evaluator.get_best_moves(board, 3)\n",
    "print(f\"Best moves: {best_moves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f434f5",
   "metadata": {},
   "source": [
    "## Load our CNN trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46049a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_model, device = RL_utils.load_model(ref_model_path)\n",
    "# reference_model = reference_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a6327",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      4\u001b[39m     output = model(dummy_input)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Should be [1, 4288]\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# dummy_input = torch.randn(1, 19, 8, 8).to(device)\n",
    "# with torch.no_grad():\n",
    "#     output = model(dummy_input)\n",
    "#     print(f\"Model output shape: {output.shape}\")  # Should be [1, 4288]\n",
    "#     print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7e154",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2570efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1000 middle game positions\n",
      "Total positions loaded: 1000\n"
     ]
    }
   ],
   "source": [
    "pgn_file = \"games/lichess_db_2016-04.pgn\"\n",
    "positions = []\n",
    "positions += RL_utils.extract_varied_positions(pgn_file, num_positions=1000)\n",
    "print(f\"Total positions loaded: {len(positions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ea359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creating a batch\n",
    "# if positions:\n",
    "#     board_tensors, legal_masks, boards = RL_utils.create_training_batch(positions, batch_size=4)\n",
    "#     print(f\"Batch shapes:\")\n",
    "#     print(f\"  Board tensors: {board_tensors.shape}\")\n",
    "#     print(f\"  Legal masks: {legal_masks.shape}\")\n",
    "#     print(f\"  Number of boards: {len(boards)}\")\n",
    "    \n",
    "#     # Show a sample position\n",
    "#     print(f\"\\nSample position FEN: {boards[0].fen()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3324c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boards[0]\n",
    "# eval_score = evaluator.evaluate_position(boards[0])\n",
    "# print(f\"Position evaluation: {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5400975",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1ab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "# Self-play\n",
    "NUM_SELF_PLAY_GAMES = 100\n",
    "MAX_GAME_MOVES = 200\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "# Model Saving\n",
    "MODEL_SAVE_PATH = \"models/dual_head_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92ff69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_self_play_data(model, device, start_positions, num_games=NUM_SELF_PLAY_GAMES):\n",
    "    \"\"\"Generate self-play data with error handling\"\"\"\n",
    "    data = []\n",
    "    successful_games = 0\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        try:\n",
    "            board = random.choice(start_positions)\n",
    "            # game_history, result = RL_utils.play_self_play_game(model, device, board, MAX_GAME_MOVES, TEMPERATURE)\n",
    "            game_history, result = RL_utils.play_game_with_mcts(model, device, board, MAX_GAME_MOVES, TEMPERATURE)\n",
    "            \n",
    "            if len(game_history) > 0:  # Only add if we have valid data\n",
    "                data.append((game_history, result))\n",
    "                successful_games += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in game {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Generated {successful_games} successful games out of {num_games} attempts\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_self_play(model, optimizer, game_histories, device):\n",
    "    \"\"\"Train model on self-play data with gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Flatten all examples from all games\n",
    "    all_examples = []\n",
    "    for history, result in game_histories:\n",
    "        for board_tensor, legal_mask, move_idx, turn in history:\n",
    "            # Convert result to value from current player's perspective\n",
    "            value = result if turn else -result\n",
    "            all_examples.append((board_tensor, legal_mask, move_idx, value))\n",
    "    \n",
    "    if len(all_examples) == 0:\n",
    "        print(\"No training examples available!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Training on {len(all_examples)} examples\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        random.shuffle(all_examples)\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for i in range(0, len(all_examples), BATCH_SIZE):\n",
    "            batch = all_examples[i:i+BATCH_SIZE]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch\n",
    "                boards = torch.cat([ex[0] for ex in batch]).to(device)\n",
    "                masks = torch.stack([ex[1] for ex in batch]).to(device)\n",
    "                move_targets = torch.tensor([ex[2] for ex in batch], dtype=torch.long).to(device)\n",
    "                value_targets = torch.tensor([ex[3] for ex in batch], dtype=torch.float).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                policy_logits, value_preds = model(boards)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, p_loss, v_loss = RL_utils.compute_loss(policy_logits, value_preds, move_targets, value_targets, masks)\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Skipping batch due to NaN loss\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                epoch_losses.append(loss.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//BATCH_SIZE}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if epoch_losses:\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: No valid batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_on_mcts(reference_model, model, optimizer, game_histories, device):\n",
    "    \"\"\"Train model on self-play data with gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Flatten all examples from all games\n",
    "    all_examples = []\n",
    "    for history, result in game_histories:\n",
    "        for board_tensor, policy_vector, turn in history:\n",
    "            # Convert result to value from current player's perspective\n",
    "            value = result if turn else -result\n",
    "            policy_tensor = torch.tensor(policy_vector, dtype=torch.float32)\n",
    "            all_examples.append((board_tensor, policy_tensor, value))\n",
    "    \n",
    "    if len(all_examples) == 0:\n",
    "        print(\"No training examples available!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Training on {len(all_examples)} examples\")\n",
    "    \n",
    "    kl_history = []\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        random.shuffle(all_examples)\n",
    "        batch_kl = []\n",
    "        batch_losses = []\n",
    "        avg_kl = None\n",
    "        \n",
    "        for i in range(0, len(all_examples), BATCH_SIZE):\n",
    "            batch = all_examples[i:i+BATCH_SIZE]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch\n",
    "                boards = torch.cat([ex[0] for ex in batch]).to(device)\n",
    "                policy_targets = torch.stack([ex[1] for ex in batch]).to(device)\n",
    "                value_targets = torch.tensor([ex[2] for ex in batch], dtype=torch.float).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                policy_logits, value_preds = model(boards)\n",
    "                with torch.no_grad():\n",
    "                    ref_policy_logits, _ = reference_model(boards)\n",
    "                    ref_policy_probs = F.softmax(ref_policy_logits, dim=1)\n",
    "\n",
    "                policy_probs = F.log_softmax(policy_logits, dim=1)\n",
    "                kl_div = F.kl_div(policy_probs, ref_policy_probs, reduction='batchmean')\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, p_loss, v_loss = RL_utils.compute_loss_mcts(policy_logits, value_preds, policy_targets, value_targets, kl_div, epoch)\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Skipping batch due to NaN loss\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_kl.append(kl_div.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//BATCH_SIZE}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_kl:\n",
    "            avg_kl = np.mean(batch_kl)\n",
    "            kl_history.append(avg_kl)\n",
    "        if batch_losses:\n",
    "            avg_loss = np.mean(batch_losses)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            if avg_kl is not None:\n",
    "                print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}, avg KL={avg_kl:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: avg loss={avg_loss:.4f}, avg KL=N/A\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}: No valid batches\")\n",
    "\n",
    "    # Plot KL-divergence history\n",
    "    plt.figure()\n",
    "    plt.plot(kl_history, label=\"KL-divergence\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"KL-divergence\")\n",
    "    plt.title(\"KL-divergence vs Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c7b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/TORCH_250EPOCH_DoubleHead.pth\n",
      "\n",
      "=== Iteration 1 ===\n"
     ]
    }
   ],
   "source": [
    "# reference_model, device = RL_utils.load_model(ref_model_path)\n",
    "# reference_model.requires_grad_(False)\n",
    "# model, _ = RL_utils.load_resnet_model()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# # Main training loop\n",
    "# for iteration in range(5):  # 5 outer iterations of self-play + training\n",
    "#     print(f\"\\n=== Iteration {iteration+1} ===\")\n",
    "    \n",
    "#     # Generate self-play games\n",
    "#     self_play_data = generate_self_play_data(model, device, positions)\n",
    "    \n",
    "#     # Train model on self-play games\n",
    "#     train_on_mcts(reference_model, model, optimizer, self_play_data, device)\n",
    "\n",
    "#     # Save model\n",
    "#     torch.save({\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#     }, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72466a52",
   "metadata": {},
   "source": [
    "# Updated Code with new style training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a399ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts import SimpleMCTS\n",
    "from chess_utils import move_to_policy_index, board_to_tensor\n",
    "from RL_utils import blend_policies, policy_index_to_move, get_game_result\n",
    "\n",
    "def generate_self_play_data_with_blending(model, reference_model, device, start_positions, num_games=100, N_OPENING_MOVES=10):\n",
    "    data = []\n",
    "    for i in range(num_games):\n",
    "        board = random.choice(start_positions)\n",
    "        mcts = SimpleMCTS(model, device, num_simulations=400)\n",
    "        game_history = []\n",
    "        move_count = 0\n",
    "        while not board.is_game_over() and move_count < MAX_GAME_MOVES:\n",
    "            temperature = 1.0 if move_count < N_OPENING_MOVES else 0.1\n",
    "            action_probs, root = mcts.get_action_probs(board, temperature)\n",
    "            # Convert MCTS action_probs dict to vector\n",
    "            mcts_policy = np.zeros(4288)\n",
    "            for move, prob in action_probs.items():\n",
    "                idx = move_to_policy_index(move)\n",
    "                mcts_policy[idx] = prob\n",
    "\n",
    "            # Get CNN policy\n",
    "            with torch.no_grad():\n",
    "                board_tensor = board_to_tensor(board)\n",
    "                input_tensor = torch.tensor(board_tensor, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                cnn_logits, _ = reference_model(input_tensor)\n",
    "                cnn_policy = torch.softmax(cnn_logits, dim=1).cpu().numpy()[0]\n",
    "                # Mask illegal moves\n",
    "                legal_indices = [move_to_policy_index(move) for move in board.legal_moves]\n",
    "                mask = np.zeros_like(cnn_policy)\n",
    "                mask[legal_indices] = 1\n",
    "                cnn_policy *= mask\n",
    "                cnn_policy /= cnn_policy.sum() if cnn_policy.sum() > 0 else 1\n",
    "\n",
    "            # Blend for opening moves\n",
    "            if move_count < N_OPENING_MOVES:\n",
    "                blended_policy = blend_policies(mcts_policy, cnn_policy, alpha=0.7)\n",
    "            else:\n",
    "                blended_policy = mcts_policy\n",
    "\n",
    "            # Select move from blended policy\n",
    "            move_idx = np.random.choice(len(blended_policy), p=blended_policy)\n",
    "            move = policy_index_to_move(move_idx, board)\n",
    "            board.push(move)\n",
    "\n",
    "            # Store training data\n",
    "            board_tensor = torch.tensor(board_to_tensor(board), dtype=torch.float32).unsqueeze(0)\n",
    "            game_history.append((board_tensor, blended_policy, board.turn))\n",
    "            move_count += 1\n",
    "\n",
    "        # Get game result\n",
    "        result = get_game_result(board)\n",
    "        data.append((game_history, result))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b577c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model, device = RL_utils.load_resnet_model(ref_model_path)\n",
    "reference_model.requires_grad_(False)\n",
    "reference_model.eval()\n",
    "model, _ = RL_utils.load_resnet_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\n=== Iteration {iteration+1} ===\")\n",
    "    # Generate self-play games with policy blending\n",
    "    self_play_data = generate_self_play_data_with_blending(model, reference_model, device, positions, num_games=NUM_SELF_PLAY_GAMES, N_OPENING_MOVES=10)\n",
    "    # Train model on self-play games (with KL loss for opening positions)\n",
    "    train_on_mcts(reference_model, model, optimizer, self_play_data, device)\n",
    "    torch.save({'model_state_dict': model.state_dict()}, MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-ai-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
